# Big-Data-Analytics
This repository demonstrates big data processing, visualization, and machine learning using tools such as Hadoop, Spark, Kafka, and Python.

<img src = "https://th.bing.com/th/id/R.8332a2c65eeaecfb365ec3a11e9c2b0e?rik=a86A6oZLes5OWw&riu=http%3a%2f%2ftimesquareit.com%2fimages%2fsl-1.jpg&ehk=VKCM0JR5%2b2hM2HSb%2b%2f6w88WsQFhqxkY3pnZymVms7mo%3d&risl=&pid=ImgRaw&r=0">

## Python

Python is a high-level, interpreted programming language known for its readability and versatility. It is widely used in data science for tasks such as data manipulation, analysis, and visualization. Libraries such as Pandas, Matplotlib, and Scikit-Learn provide powerful tools for handling and analyzing large datasets.

## Hadoop

Hadoop is an open-source framework that allows for the distributed processing of large datasets across clusters of computers using simple programming models. It is designed to scale up from a single server to thousands of machines, each offering local computation and storage. Hadoop's core components include the Hadoop Distributed File System (HDFS) for storage and MapReduce for processing data.

## MapReduce

MapReduce is a programming model used for processing and generating large datasets with a parallel, distributed algorithm on a cluster. The model consists of two main tasks:
1. **Map:** Processes input data and produces intermediate key-value pairs.
2. **Reduce:** Merges all intermediate values associated with the same key and outputs the final result.

### Apache Hive

Apache Hive is a data warehousing and SQL-like query language for Hadoop. It provides a high-level abstraction over Hadoop's complexity by allowing users to write SQL queries (HiveQL) to interact with data stored in HDFS. Hive is designed for batch processing and is suitable for querying and managing large datasets efficiently, making it a popular choice for data analysis in big data ecosystems.

### Apache Spark

Apache Spark is a fast, open-source processing engine designed for large-scale data processing. It offers high-level APIs in multiple programming languages and provides built-in modules for SQL, streaming, machine learning, and graph processing. Spark's Resilient Distributed Datasets (RDDs) allow for in-memory data processing, significantly speeding up the execution of data workflows compared to traditional disk-based systems like Hadoop's MapReduce.

### Apache Kafka

Apache Kafka is a distributed streaming platform that enables real-time data pipelines and streaming applications. It is designed for high throughput and fault tolerance, allowing users to publish and subscribe to streams of records in a scalable way. Kafka is often used for building real-time analytics and event-driven architectures, making it an essential component for applications that require processing and analyzing continuous streams of data.
Here are separate descriptions for Matplotlib and Seaborn:

## Matplotlib

Matplotlib is a comprehensive plotting library for Python that allows users to create static, animated, and interactive visualizations in a variety of formats. It provides a wide range of customizable options for visual representation, making it suitable for producing complex plots such as line graphs, scatter plots, bar charts, histograms, and heatmaps. Matplotlib is often used in data analysis and scientific computing, allowing users to visualize their data and results effectively.

## Seaborn

Seaborn is a statistical data visualization library based on Matplotlib that provides a high-level interface for creating attractive and informative graphics. It simplifies the process of creating complex visualizations such as categorical plots, heatmaps, and time series data, with built-in themes and color palettes for enhanced aesthetics. Seaborn is designed to work seamlessly with Pandas data structures, allowing for easy manipulation and visualization of data. 

---

## Experiments

### 1. Hadoop Installation

**Description:**  
This experiment involves the installation and setup of Hadoop on your system. It covers the necessary configurations to get Hadoop up and running, allowing you to explore its capabilities for handling large-scale data processing tasks.

### 2. Data Exploration with Hadoop

**Description:**  
In this experiment, we will use Hadoop to explore large-scale datasets stored in the Hadoop Distributed File System (HDFS). You will perform basic operations such as listing files, reading data, and calculating summary statistics to understand the structure and content of the datasets.

### 3. SQL Queries with Hive

**Description:**  
In this experiment, you will use Apache Hive, a data warehousing tool built on top of Hadoop, to run SQL queries on datasets stored in HDFS. You will explore various SQL operations such as filtering, joining, and aggregating large datasets to extract meaningful insights and understand the integration of SQL with big data.

### 4. Word Count with MapReduce

**Description:**  
This experiment implements the classic MapReduce word count algorithm to count the frequency of words in a large text corpus stored in HDFS. You will learn to structure Map and Reduce functions, demonstrating the power of MapReduce for processing and analyzing large volumes of text data.

### 5. Data Analysis with Apache Spark

**Description:**  
In this experiment, you will utilize Apache Spark to analyze large datasets by loading them into Spark Resilient Distributed Datasets (RDDs). You will perform operations such as filtering, mapping, and aggregation, showcasing Spark's ability to process big data efficiently and effectively.

### 6. Streaming Analytics with Kafka and Spark

**Description:**  
This experiment involves setting up a data streaming pipeline using Apache Kafka to ingest real-time data. You will process this data using Apache Spark Streaming for real-time analytics, demonstrating the synergy between Kafka and Spark in handling live data feeds and generating insights on-the-fly.

### 7. Data Visualization with Python and Matplotlib

**Description:**  
In this experiment, you will use Python along with the Matplotlib library to visualize insights extracted from large datasets. You will create various types of plots, such as histograms, scatter plots, and time series visualizations, to effectively communicate data findings and trends.

---

## Additional Resources

- **Detailed Documentation:** A Word document explaining each command used in the experiments.
- **Output Document:** A document showcasing the outputs and results for each experiment.

## Thanks for Visiting üòÑ

- Drop a üåü if you find this repository useful.<br><br>
- If you have any doubts or suggestions, feel free to reach me.<br><br>
üì´ How to reach me:  &nbsp; [![Linkedin Badge](https://img.shields.io/badge/-madhurima-blue?style=flat&logo=Linkedin&logoColor=white)](https://www.linkedin.com/in/madhurima-rawat/) &nbsp; &nbsp;
<a href ="mailto:rawatmadhurima@gmail.com"><img src="https://github.com/madhurimarawat/Machine-Learning-Using-Python/assets/105432776/b6a0873a-e961-42c0-8fbf-ab65828c961a" height=35 width=30 title="Mail Illustration" alt="Mail Illustrationüì´" > </a><br><br>
- **Contribute and Discuss:** Feel free to open <a href= "https://github.com/madhurimarawat/Big-Data-Analytics/issues">issues üêõ</a>, submit <a href = "https://github.com/madhurimarawat/Big-Data-Analytics/pulls">pull requests üõ†Ô∏è</a>, or start <a href = "https://github.com/madhurimarawat/Big-Data-Analytics/discussions">discussions üí¨</a> to help improve this repository!
